# -*- coding: utf-8 -*-
"""main_part_a.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SE5WZrsbeI3DeMTRktkSYIUjumnBxI7q

# MA23M002 - ABHINAV T K
# CS6910 - Assignment 2

**Github repo:** https://github.com/abhinavtk1/CS6910-A2 <br>
**Wandb report:** https://wandb.ai/abhinavtk/MA23M002-A2/reports/MA23M002-Assignment-2--Vmlldzo3MzAyNDA0
"""

#!pip install wandb

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import cv2
import matplotlib.image as mpimg
import wandb
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torch.utils.data import random_split
from torchvision import transforms, datasets
# Dataset google drive link: https://drive.google.com/drive/folders/1vJqUoTLPd-uXgYR4Kc0vsnisZcF_AJtp?usp=sharing
train_dir = '/content/drive/MyDrive/CS6910-A2/CS6910-A2/train'
test_dir = '/content/drive/MyDrive/CS6910-A2/CS6910-A2/val'
# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#print(device)

# Use ImageFolder to create a dataset
def prep_dataset():
  # Transformations to apply to the images
  # Normalize the images
  transform = transforms.Compose([
  transforms.Resize(224),  # Resize the images
  transforms.ToTensor(),        # Convert images to PyTorch tensors
  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])
  trainset = datasets.ImageFolder(root=train_dir, transform=transform)
  val_size = int(0.2 * len(trainset))
  train_size = len(trainset) - val_size
  trainset, valset = random_split(trainset, [train_size, val_size])
  testset = datasets.ImageFolder(root=test_dir, transform=transform)
  return trainset, valset, testset  # returns train, validation and test datasets

# Reference: https://machinelearningmastery.com/building-a-convolutional-neural-network-in-pytorch/

class CNN(nn.Module):
  def __init__(self, trainset, valset, batch_size, num_filters,filter_org, kernel_size,
                act_fn, num_neurons, batch_norm, dropout_rate):
    super().__init__()

    # DataLoader for the dataset
    self.dataloader_train = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle=True)
    self.dataloader_val = torch.utils.data.DataLoader(valset, batch_size = batch_size, shuffle=True)
    #dataloader_test = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle=True)
    self.batch_norm = batch_norm
    #num_layers = 5
    filters = []
    if filter_org=='same':
        filters = [num_filters]*5
    elif filter_org=='double':
        filters = [num_filters, num_filters*2, num_filters*4, num_filters*8, num_filters*16]
    elif filter_org=='half':
        filters = [num_filters, num_filters//2, num_filters//4, num_filters//8, num_filters//16]
    #Conv layer 1
    self.conv1 = nn.Conv2d(3, filters[0],
                            kernel_size=(kernel_size[0],kernel_size[0]),
                            stride=1, padding='same')
    #x = (256 - kernel_size[0] + 2)/1 + 1
    x = 224           # img size
    self.bn1 = nn.BatchNorm2d(filters[0])  # Batch Normalization

    if act_fn=='relu':          #nn.ReLU nn.GELU nn.SiLU nn.Mish nn.Tanh
      self.act1 = nn.ReLU()
    elif act_fn=='tanh':
      self.act1 = nn.Tanh()
    elif act_fn=='gelu':
      self.act1 = nn.GELU()
    elif act_fn=='silu':
      self.act1 = nn.SiLU()
    elif act_fn=='mish':
      self.act1 = nn.Mish()
    self.dropout1 = nn.Dropout(p=dropout_rate)
    self.pool1 = nn.MaxPool2d(2,2)
    x = x//2

    #Conv layer 2
    self.conv2 = nn.Conv2d(filters[0], filters[1],
                            kernel_size=(kernel_size[1],kernel_size[1]),
                            stride=1, padding='same')
    self.bn2 = nn.BatchNorm2d(filters[1])

    if act_fn=='relu':
      self.act2 = nn.ReLU()
    elif act_fn=='tanh':
      self.act2 = nn.Tanh()
    elif act_fn=='gelu':
      self.act2 = nn.GELU()
    elif act_fn=='silu':
      self.act2 = nn.SiLU()
    elif act_fn=='mish':
      self.act2 = nn.Mish()
    self.dropout2 = nn.Dropout(p=dropout_rate)
    self.pool2 = nn.MaxPool2d(2,2)
    x = x//2

    #Conv layer 3
    self.conv3 = nn.Conv2d(filters[1], filters[2],
                            kernel_size=(kernel_size[2],kernel_size[2]),
                            stride=1, padding='same')

    self.bn3 = nn.BatchNorm2d(filters[2])
    if act_fn=='relu':
      self.act3 = nn.ReLU()
    elif act_fn=='tanh':
      self.act3 = nn.Tanh()
    elif act_fn=='gelu':
      self.act3 = nn.GELU()
    elif act_fn=='silu':
      self.act3 = nn.SiLU()
    elif act_fn=='mish':
      self.act3 = nn.Mish()
    self.dropout3 = nn.Dropout(p=dropout_rate)
    self.pool3 = nn.MaxPool2d(2,2)
    x = x//2

    #Conv layer 4
    self.conv4 = nn.Conv2d(filters[2], filters[3],
                            kernel_size=(kernel_size[3],kernel_size[3]),
                            stride=1, padding='same')
    self.bn4 = nn.BatchNorm2d(filters[3])
    if act_fn=='relu':
      self.act4 = nn.ReLU()
    elif act_fn=='tanh':
      self.act4 = nn.Tanh()
    elif act_fn=='gelu':
      self.act4 = nn.GELU()
    elif act_fn=='silu':
      self.act4 = nn.SiLU()
    elif act_fn=='mish':
      self.act4 = nn.Mish()
    self.dropout4 = nn.Dropout(p=dropout_rate)
    self.pool4 = nn.MaxPool2d(2,2)
    x = x//2

    #Conv layer 5
    self.conv5 = nn.Conv2d(filters[3], filters[4],
                            kernel_size=(kernel_size[4],kernel_size[4]),
                            stride=1, padding='same')
    self.bn5 = nn.BatchNorm2d(filters[4])
    if act_fn=='relu':
      self.act5 = nn.ReLU()
    elif act_fn=='tanh':
      self.act5 = nn.Tanh()
    elif act_fn=='gelu':
      self.act5 = nn.GELU()
    elif act_fn=='silu':
      self.act5 = nn.SiLU()
    elif act_fn=='mish':
      self.act5 = nn.Mish()
    self.dropout5 = nn.Dropout(p=dropout_rate)
    self.pool5 = nn.MaxPool2d(2,2)
    x = x//2
    x = x*x*filters[4]
    #print(x)

    # Flatten
    self.flatten = nn.Flatten()

    # dense layer
    self.fc6 = nn.Linear(x, num_neurons)
    if act_fn=='relu':
      self.act6 = nn.ReLU()
    elif act_fn=='tanh':
      self.act6 = nn.Tanh()
    elif act_fn=='gelu':
      self.act6 = nn.GELU()
    elif act_fn=='silu':
      self.act6 = nn.SiLU()
    elif act_fn=='mish':
      self.act6 = nn.Mish()
    self.dropout6 = nn.Dropout(p=dropout_rate)
    # output layer
    self.fc7 = nn.Linear(num_neurons, 10)


  def forward(self, x):
    # input 3x128x128, output 32x128x128
    x = self.conv1(x)
    if self.batch_norm:
        x = self.bn1(x)
    x = self.act1(x)
    x = self.dropout1(x)
    # input 32x128x128, output 32x64x64
    x = self.pool1(x)

    # input 32x64x64, output 32x64x64
    x = self.conv2(x)
    if self.batch_norm:
        x = self.bn2(x)
    x = self.act2(x)
    x = self.dropout2(x)
    # input 32x64x64, output 32x32x32
    x = self.pool2(x)

    # input 32x32x32, output 64x32x32
    x = self.conv3(x)
    if self.batch_norm:
        x = self.bn3(x)
    x = self.act3(x)
    x = self.dropout3(x)
    # input 64x32x32, output 64x16x16
    x = self.pool3(x)

    # input 64x16x16, output 64x16x16
    x = self.conv4(x)
    if self.batch_norm:
        x = self.bn4(x)
    x = self.act4(x)
    x = self.dropout4(x)
    # input 64x16x16, output 64x8x8
    x = self.pool4(x)

    # input 64x8x8, output 128x8x8
    x = self.conv5(x)
    if self.batch_norm:
        x = self.bn5(x)
    x = self.act5(x)
    x = self.dropout5(x)
    # input 128x8x8, output 128x4x4
    x = self.pool5(x)

    # input 128,4,4 output 2048
    x = self.flatten(x)

    # input 2048, output 512
    x = self.act6(self.fc6(x))
    x = self.dropout6(x)

    # input 512, output 10
    x = self.fc7(x)
    return x

  def train(self, model, loss_fn, optimizer, n_epochs = 10):
    for epoch in range(n_epochs):
      train_accuracy = 0
      count = 0
      for inputs, labels in self.dataloader_train:
        # forward, backward, and then weight update
        inputs = inputs.to(device)
        labels = labels.to(device)
        # Forward pass
        y_pred = model(inputs)
        loss = loss_fn(y_pred, labels)
        train_accuracy += (torch.argmax(y_pred, 1) == labels).float().sum()
        count += len(labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
      train_accuracy /= count
      print("Epoch %d: model train accuracy %.2f%%" % (epoch, train_accuracy*100))
      wandb.log({ 'epoch': epoch, 'train_accuracy': train_accuracy * 100})

      val_accuracy = 0
      count = 0
      for inputs, labels in self.dataloader_val:
        inputs = inputs.to(device)
        labels = labels.to(device)
        y_pred = model(inputs)
        val_loss = loss_fn(y_pred, labels)
        val_accuracy += (torch.argmax(y_pred, 1) == labels).float().sum()
        count += len(labels)
      val_accuracy /= count
      print("Epoch %d: model validation accuracy %.2f%%" % (epoch, val_accuracy*100))
      wandb.log({ 'epoch': epoch, 'validation_accuracy': val_accuracy * 100})
      wandb.log({ 'epoch': epoch, 'validation_loss': val_loss * 100})
      return model



def main():
  with wandb.init() as run:
    run_name = "-f_num_"+str(wandb.config.filters_num)+"-f_num_"+wandb.config.filter_org+"-ac_fn_"+wandb.config.act_fn+\
                "-b_norm_"+str(wandb.config.batch_norm) + "-bs_"+str(wandb.config.batch_size) +"-neu_num"+str(wandb.config.num_neurons_dense)

    wandb.run.name = run_name
    trainset, valset, testset = prep_dataset(train_dir, test_dir)
    model = CNN(trainset, valset, batch_size=wandb.config.batch_size, num_filters = wandb.config.filters_num,
                filter_org =wandb.config.filter_org , kernel_size = wandb.config.kernel_size, act_fn = wandb.config.act_fn,
                num_neurons = wandb.config.num_neurons_dense,batch_norm = wandb.config.batch_norm, dropout_rate=wandb.config.dropout)
    model.to(device)
    loss_fn = nn.CrossEntropyLoss()
    #optimizer = optim.SGD(model.parameters(), lr=wandb.config.learning_rate, momentum=0.9)
    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=wandb.config.l2_reg)
    model.train(model, loss_fn, optimizer, n_epochs = 10) # wandb.config.epochs)

if __name__ == '__main__':
  classes = ['Amphibia','Animalia','Arachnida','Aves','Fungi',
           'Insecta','Mammalia','Mollusca','Plantae','Reptilia']
  # Connect to google drive
  #from google.colab import drive
  #drive.mount('/content/drive')

  #train_dir = '/kaggle/input/inaturalist/inaturalist_12K/train'
  #test_dir = '/kaggle/input/inaturalist/inaturalist_12K/val'

  IMG_SIZE = (224,224)
  # sweep config file
  sweep_config = {
      'method': 'bayes',
      'name' : 'bayes sweep',
      'metric': {
        'goal': 'maximize',
        'name': 'validation_accuracy'
      },
      'parameters': {
          'filters_num':{
              'values': [32, 64, 128]
          },
          'filter_org':{
            'values': ['same', 'double', 'half']
          },
          'act_fn': {
              'values': ['relu', 'gelu', 'silu', 'mish', 'tanh']
          },
          'data_aug':{
              'values': [False]
          },
          'batch_norm':{
              'values': [True, False]
          },
          'dropout': {
              'values': [0, 0.2, 0.5]
          },
          'learning_rate': {
              'values': [1e-3, 1e-4]
          },
          'l2_reg':{
              'values': [0, 0.0005, 0.05]
          },
          'batch_size': {
              'values': [32, 64]
          },
          'kernel_size':{
              'values': [[3]*5, [3, 5, 5, 7, 7], [5]*5, [7]*7, [7, 5, 5, 3, 3] ]
          },
          'num_neurons_dense':{
              'values': [64, 128, 256]
          },
          'epochs':{
              'values': [10]
          }
      }
  }
  # Create a sweep
  sweep_id = wandb.sweep(sweep = sweep_config, entity="abhinavtk", project='MA23M002-A2')
  wandb.agent(sweep_id, function = main, count = 50) 
  wandb.finish()

